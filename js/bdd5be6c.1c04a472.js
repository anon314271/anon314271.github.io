(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["bdd5be6c"],{"2fde":function(e,n,t){"use strict";t.r(n);var o=function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("q-page",[t("q-parallax",{attrs:{height:150,speed:.5},scopedSlots:e._u([{key:"media",fn:function(){return[t("img",{attrs:{src:e.post.bg}})]},proxy:!0}])},[t("div",{staticClass:"text-h4 caption"},[e._v(e._s(e.post.title))])]),t("div",{staticClass:"row"},[t("div",{staticClass:"col-2 q-pa-md gt-sm"},[t("q-list",e._l(e.toc,function(n){return t("q-item",{key:n.id,attrs:{clickable:""},on:{click:function(t){return e.scrollTo(n.id)}}},[t("q-item-section",{staticClass:"text-body1",style:{marginLeft:7*(n.level-1)+"%"}},[e._v("\n              "+e._s(n.label)+"\n            ")])],1)}),1)],1),t("div",{staticClass:"col q-pt-lg"},[t("div",{staticClass:"row justify-center"},[t("q-markdown",{ref:"md",staticClass:"col-xs-11 col-sm-10",attrs:{src:e.post.content,toc:"","toc-start":2,"toc-end":3},on:{data:e.onToc}})],1)])]),t("q-page-scroller",{attrs:{position:"bottom-right","scroll-offset":150,offset:[18,18]}},[t("q-btn",{attrs:{fab:"",icon:"keyboard_arrow_up",color:"primary"}})],1)],1)},a=[],i=t("dc23"),s={name:"Posts",components:{},data:function(){return{markdown:"",post:"",toc:""}},methods:{onToc:function(e){this.toc=e},scrollTo:function(e){document.getElementById(e).scrollIntoView()}},created:function(){var e=this.$route.params.post_id;this.post=i[e],this.markdown=i[e].content}},r=s,l=(t("c72e"),t("2877")),d=Object(l["a"])(r,o,a,!1,null,null,null);n["default"]=d.exports},c72e:function(e,n,t){"use strict";var o=t("d56c"),a=t.n(o);a.a},d56c:function(e,n,t){},dc23:function(e){e.exports=JSON.parse('[{"title":"How this Site is Built","content":"\\nThis site is built using the [Quasar Framework](https://quasar.dev) and deployed as a single page application\\n\\n## A bit about Quasar\\n\\nThe Quasar Framework is a great tool for anyone looking to make websites, mobile apps or cross platform applications quickly. The project is opensource and has been growing rapidly over the last few years and there have been some great developments. Quasar relies heavily on VueJS in order to run so in order to get started it is worth getting to grips with Vue, from then on building Webpages is fairly trivial as there is a rich set of components to choose from and deployment is also relatively straightforward.\\n\\n\\n## Pages \\n\\nPages such as this one are written in markdown. They are then automatically bundled together with a small script, doing so keeps everything static and means no database is needed to fetch content. It also provides a simple mechanism to update site content.\\n\\n\\nThe following script is used bundle the markdown files and add them to the static files directory.\\n\\n\\n```\\n// buildcontent.js\\n\\nconst fs = require(\\"fs\\")\\nconst path = require(\\"path\\")\\n\\nconst markdown_dir = \\"./markdown\\"\\n\\nconst markdown_files = fs.readdirSync(markdown_dir)\\nconst index = []\\nconst articles = []\\nlet filePath\\n\\nmarkdown_files.forEach( (file, i) => {\\n\\n\\tfilePath = path.join(__dirname, markdown_dir, file); \\n\\n\\tlet content = fs.readFileSync(filePath, {encoding: \'utf-8\'})\\n\\n\\tdate = file.slice(0,8)\\n\\tfile = file.replace(/\\\\d{8}_/, \\"\\") \\n\\tfile = file.replace(/\\\\.[^/.]+$/, \\"\\")\\n\\t\\n\\t// add articles to index\\n\\tindex.push({\\n\\t\\ttitle: file,\\n\\t\\tid: i,\\n\\t\\tbg: \\"statics/post_bg/\\" + date + \\".jpg\\",\\n\\t\\tdate: {\\n\\t\\t\\tday: date.slice(6,8),\\n\\t\\t\\tmonth: date.slice(4,6),\\n\\t\\t\\tyear: date.slice(0,4)\\n\\t\\t} \\n\\t})\\n\\n\\t// add content \\n\\tarticles.push({\\n\\t\\ttitle: file,\\n\\t\\tcontent: content,\\n\\t\\tbg: \\"statics/post_bg/\\" + date + \\".jpg\\"\\n\\t})\\n})\\n\\nfs.writeFileSync(\'./src/statics/index.json\', JSON.stringify(index))\\nfs.writeFileSync(\'./src/statics/content.json\', JSON.stringify(articles))\\n```\\n<br>\\n \\nIn order to run the script node is needed. Node is a Javascript runtime based off of Chromes V8 engine. It was created in order to run Javascript code on a local computer. Typically Javascript is only used to run webpages but with the introduction of node, javacript is being run on a local machine with access to streams such as stdin, stderr and so on. Node is typically used in conjunction with the development of webpages in order to host and serve the pages; a popular framework used to do so is Express. For my purposes I am using node in order to write the buffer to a new file know as `index.json` in turn the file is saved in the static directory of the webpage.\\n\\n```\\n$ node buildcontent.js\\n```\\n\\n<br>\\n","bg":"statics/post_bg/20190522.jpg"},{"title":"Basics of WebRTC","content":"\\nWebRTC allows for communication be it text, video or audio between peers and without the need for complex server infrastructure.\\n\\nWebRTC is natively available in:\\n - Modern Browsers\\n - Android\\n - iOS\\n\\n ## How WebRTC began\\n\\nGoogle bought GIPS which had developed components of RTC. They then open sourced the codecs and techniques developed by the company. Google engaged with industry standards setters to ensure consensus and began the implementation in 2011.\\n\\n\\n## Main APIs\\n\\nThere are three main APIs involved with WebRTC: \\n[MediaDevices]( https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)\\n[RTCPeerConnection](https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection)\\n[RTCDataChannel](https://developer.mozilla.org/en-US/docs/Web/API/RTCDataChannel)\\n\\n\\n###  MediaDevices.getUserMedia()\\n Prompts the user for access to their media streams (video and audio) and produces a MediaStream containing the requested media.\\n\\n\\n### MediaStream\\n\\nRepresents a synchronised media stream. i.e. Video and audio\\n\\n### RTCPeerConnection \\n\\nA class used to represent a WebRTC connection between a local computer and peer\\n\\n\\n### RTCDataChannel\\n\\nAssociated with a given RTCPeerConnection instance. Represents a network channel used for p2p data transfer\\n\\n\\n","bg":"statics/post_bg/20190523.jpg"},{"title":"Python Virtual Environments","content":"\\nVirtual environments also know as venvs or envs are used to isolate your projects dependencies from the rest of the packages you might have installed on your machine.\\n\\n## Why use them\\n\\nVEnvs when you\'re first getting started seem like a waste of time, they often require some setup and typically you just want to get up and running with what you are trying to do. The reason venvs are worth using is that they save you from a huge headache in the future. Most of the time you\'ll end up doing multiple projects on a single machine and as your project evolves so do the requirements of that project, for instance at some point you may need an external package in order to add some functionality, installing the package into your global environment is a hassle free short term solution, issues start to arise when you have multiple projects relying on a single package. For instance you may have developed a django app when it was in version 2.1, and now want to develop another django app using version 2.2 which has some additional features and is the newest LTS release, when you go back to work on your old project you\'ll find some things have broken. Things can quickly get out of hand with multiple projects and multiple overlapping dependencies leading to long term issues that could take hours to fix.\\n\\nVirtual environments also allow a project to become more portable, by that I mean if you want to share your codebase along with the packages you are using with a friend or coworker you can quickly run the following command:\\n\\n```\\n$ pip freeze >> requirements.txt \\t\\n```\\n\\n<br>\\n\\nthis adds a list of the packages you are using to a file called `\\"requirements.txt\\"` this can be added to the repository and then when it is shared only the relevant files will be downloaded, and not everything on your system. \\n\\nIn addition your requirements can be tracked via git and so you can have a log of all the changes you made when your project evolves.\\n\\n\\n## Using Virtual Environments\\n\\nAs of Python 3.6 the venv module is included as part of the standard library, previous to that you would have to use the virtualenv package available on pip.\\n\\nIn order to get started run:\\n\\n```\\n$ python -m venv env\\n```\\n\\n<br>\\n\\nYou\'ve just created a virtual environment, before adding any packages make sure to run\\n\\n```\\n$ . env/bin/activate\\n```\\n\\n<br>\\n\\nYou\'ll now see the following tag before on your terminal `(env) $ `\\n\\nFrom here onwards you\'re able to add pip packages as you do normally!\\n\\nIf you want to install packages from another repo you can run:\\n\\n```\\npip install -r requirements.txt\\n```\\n\\n<br>\\n","bg":"statics/post_bg/20190606.jpg"},{"title":"Making a Search Bar in 1 line of Bash","content":"\\nBash is a great tool if your looking to automate things or get things done efficiently. \\n\\nWhen working with BASH its important to keep in mind the UNIX philosophy \\n\\n> Write programs that do one thing and do it well.\\n> Write programs to work together.\\n> Write programs to handle text streams, because that is a universal interface.\\n\\nI say this because on most UNIX based or POSIX compliant systems such as MacOSX or mostly POSIX compliant system such as GNU/Linux there are a great number of tools that come with the OS, for instance you have well documented programs like: awk, sed, grep, head, tail, cat and so on which provide some great functionality. These programs are typically written in C and have been around for decades so you can be sure they run fast.\\n\\n## The UNIX Pipeline \\n\\nOne thing that needs to be mentioned is that pipes are used. Pipes or the Unix Pipeline was invented by Douglas McIlroy, it was implemented in 1973 by Ken Thompson. Pipes on the command line are denoted as `|` they take output from one program (stdout) and pass it as input (stdin) to the next one. Allowing you to chain commands from left to right. With pipes in mind we can get on to building our search bar.\\n\\n## Prerequisites\\n\\nBefore we can search through the files an index or database of all the files, for this updatedb is used. It comes installed on most systems, so all you need to run is\\n\\n```\\n$ sudo updatedb\\n```\\n\\n<br>\\n\\nIf you want a new database to be made at the end of each day you can run\\n\\n```\\n$ sudo systemctl start updatedb\\n```\\n\\n<br>\\n\\nalternatively a cronjob can be added in order to run updatedb whenever you like, I\'ll cover CRON in a future post.\\n\\nIn addition to updatedb, locate is needed. This can be installed with your package manager, for example on Debian you can run `sudo apt-get install locate`. For other systems such as arch you can run `sudo pacman -S mlocate` or CentOS `yum install locate`.\\n\\n## Building a search bar\\n\\nNow the following commands can be chained together:\\n\\n```\\nlocate $HOME \\n``` \\n\\n<br>\\n\\nProduces a list of all the files in the users home directory, this is then piped to\\n\\n```\\nrofi -dmenu\\n``` \\n\\n<br>\\n\\nmakes a searchable menu allowing the user to search and select a file or folder, when a choice is made it is piped to\\n\\n```\\nxargs -r xdg-open\\n``` \\n\\n<br>\\n\\nWhich opens the selected file based on its MIME type (the type of file it is, i.e. for images like png, jpgs, gifs your image viewer is used, for folders your file navigator).\\n\\n\\n### Putting it all together\\n\\n\\n```\\nlocate $HOME | rofi -dmenu | xargs -r xdg-open\\n```\\n\\n<br>\\n\\nIn one line of BASH you can have a search bar.\\n\\n\\n## Closing Remarks\\n\\nSo in one line we\'ve got a search bar, this is a feature I use a lot on my PC, I often need to search for a document or some music and its a nice convenience to have. But this post in truth is about the pipe, its a nice feature to have and there are loads of creative uses for it. The pipe is a great stepping stone to learning about UNIX and the design ideas behind it, there are some great ideas that went into the development of UNIX and baring them in mind can really help structure complex ideas in a simple set of steps.\\n\\n","bg":"statics/post_bg/20190619.jpg"},{"title":"Automating with Make","content":"\\n`make` is a great utility that comes with GNU/Linux. It allows for a series of commands to be executed automatically without the need for user input. This  does sound really similar to a BASH Script; however the difference is that `make` allows you to specify multiple functions which run their own set of commands. For instance you may have an application you just developed which you want to run on another machine; therefore you would need a way to have instructions which install and unistall the app. Makefiles can do just that, you can have a command that can install the program and another that can uninstall it.\\n\\nThe advantage of `make` is that it gives the miscellaneous functions your program may need a definitive structure. Additionally those functions can exist in a single file. Furthermore makefiles offer portability as once you know how to write and use them you\'re open to a world of makefiles which are used unanimously by others (at least in the world of Linux users).\\n\\n## The Basics\\n\\nIn order to use `make` a `makefile` is needed this is a file a named `makefile`. `make` automatically reads a makefile and follows the instructions of the functions within it. There are ways to specify which function to follow but this is beyond the scope of this post.\\n\\nA basic example of a makefile is:\\n\\n```\\nhello_world:\\n\\t    echo \\"hello world\\"\\n\\t    @echo \\"hello world\\"\\n```\\n\\n<br>\\n\\nWhen make is called the function is run and the commands are executed. In the case of commands with a `@` before them, they run silently i.e. the command is not printed only the commands output.\\n\\n## How I use make to build this site.\\n\\nI use `make` in order to build and deploy this website. Which is great because I have to run one command and all the hard work is done for me.\\n\\nThe makefile I use is:\\n\\n```{bash}\\nupdate:\\n\\t    @echo \\"bundling markdown\\"\\n\\t    node buildmd.js\\n\\t    @echo \\"building source\\"\\n\\t    quasar build\\n\\t    @echo \\"updating website\\"\\n\\t    rm -rf ../mkomod.github.io/*\\t# clears old dir\\n\\t    cp -r ./dist/spa/* ../mkomod.github.io # copies over files\\n\\t    cd ../mkomod.github.io; \\\\\\n\\t    git add . ;\\\\\\n\\t    git commit -m \\"automatic update\\"; \\\\\\n\\t    git push; # deploys to github\\n```\\n\\n## Closing remarks\\n\\nI hope you see how useful `make` is, the functionality it offers goes beyond the deployment of websites. I\'d recommend starting at makes man pages to learn more about it, there are some great tutorials around for further functionality but I hope what we talked about can get you started and save you time. Enjoy!\\n","bg":"statics/post_bg/20190826.jpg"},{"title":"The Command Line","content":"The terminal or command line can do some amazing things, I started using the command line a while ago and I don\'t think I can go back to doing some of the things I did through GUI applications.\\n\\n## The argument against  GUIs\\n\\nOne of the biggest issues with a GUI is you\'re forced to use an application in the manner the developers and designers intended. Which isn\'t always bad however you lose the intellectual freedom to do what you like with the application or change the way it behaves. For CLIs  this is completely different, they are intended to be combined to do what you want. And this boils down to the UNIX philosophy:\\n\\n> Write programs that do one thing and do it well.\\n> Write programs to work together.\\n> Write programs to handle text streams.\\n\\nCLIs are typically designed with these guidelines in mind.\\n\\n## Where GUIs outperform CLIs\\n\\nIts not all bad for GUIs for instance when browsing the internet, editing videos, or more generally when interacting with content that is visual the GUI is the right tool for the job. Of course the terminal  can be used to do something like image editing but its only good for simple stuff, for instance there\'s a great tool called `imagemagick`, it has a load of functionality but its great for quick tasks like converting an image, for example:\\n\\n```\\n$ magick foo.jpg foo.png\\n```\\n<br>\\n\\nquickly converts an image from jpg to png, which saves a bit of time when using figures in LaTeX.\\n\\n## The Caveat\\n\\nUnlike GUIs there is a learning curve when it comes to using terminal apps, with a GUI you can get up and running within a few minutes, there is however a gradual and albeit slow process to learning how to use CLIs. The difference is the learning experience for a CLI is one that generalizes; once you learn how to use a few a general understanding forms. This effect is compounding so after some time, it feels natural using the terminal.\\n\\n## Terminals, the Web browser for your machine\\n\\nMuch like how web browsers open you up to a plethora of online information and tools, the terminal opens you up to a treasure trove of applications. Learning to use the command line is like entering a whole new world, one where you have access to the tools you need as well as an endless source of knowledge.\\n\\nOver time the terminal starts to factor into your work flow and allows you to make custom tooling. For example, before writing this I made a small script that creates files with the date preceding them:\\n\\n```{sh}\\n#!/bin/sh\\n\\ntodays_date=$(date +\\"%Y%m%d\\")\\n\\n# loop through all file names given as args\\nfor file_name in \\"$@\\"\\ndo\\n    touch \\"${todays_date}_${file_name}\\"\\ndone\\n```\\n\\n<br>\\n\\nIn fairness this doesn\'t seem very useful on its own, why not just enter the date manually? \\n\\nWell, this is something I do a lot and so automating it will save some time, however the benefit comes from not having to think about something this trivial. With this small script the date will always be correct and it will always be formatted exactly as specified. It took a couple of minutes to make and its something I\'ll use for a while.\\n\\n## Closing Remarks\\n\\nThe terminal is a great tool for simple tasks, which can be combined to build something bigger. In the long run it bring some great benefits to the user, but a word of warning, its a pretty steep learning curve but a really rewarding one.\\n","bg":"statics/post_bg/20190828.jpg"}]')}}]);